pipeline_name: topic_modeling
doc: Topic-modeling of a provided corpus table dataset.
steps:
  - module_type: table.cut_column
    module_config:
      constants:
        column_name: content
    step_id: extract_texts_column
  - module_type: tokenize.texts_array
    step_id: tokenize_content
    input_links:
      texts_array: extract_texts_column.array
  - module_type: create.stopwords_list
    step_id: create_stopwords_list
  - module_type: preprocess.tokens_array
    step_id: preprocess_corpus
    input_links:
      tokens_array: tokenize_content.tokens_array
      remove_stopwords: create_stopwords_list.stopwords_list
  - module_type: generate.LDA.for.tokens_array
    step_id: generate_lda
    input_links:
      tokens_array: preprocess_corpus.tokens_array

input_aliases:
  extract_texts_column.table: corpus
  tokenize_content.tokenize_by_word: tokenize_by_word
  generate_lda.num_topics_min: num_topics_min
  generate_lda.num_topics_max: num_topics_max
  generate_lda.compute_coherence: compute_coherence
  generate_lda.words_per_topic: words_per_topic
  create_stopwords_list.languages: languages
  create_stopwords_list.stopword_lists: stopword_lists
  preprocess_corpus.to_lowercase: to_lowercase
  preprocess_corpus.remove_alphanumeric: remove_alphanumeric
  preprocess_corpus.remove_non_alpha: remove_non_alpha
  preprocess_corpus.remove_all_numeric: remove_all_numeric
  preprocess_corpus.remove_short_tokens: remove_short_tokens
  preprocess_corpus.remove_stopwords: remove_stopwords

output_aliases:
  extract_texts_column.array: content_array
  tokenize_content.tokens_array: tokenized_corpus
  preprocess_corpus.tokens_array: preprocessed_corpus
  generate_lda.topic_models: topic_models
  generate_lda.coherence_map: coherence_map
  generate_lda.coherence_table: coherence_table


defaults:
  languages: ["italian"]
  stopword_lists: []
  num_topics_min: 7
  num_topics_max: 9
  compute_coherence: true
